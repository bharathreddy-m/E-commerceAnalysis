# -*- coding: utf-8 -*-
"""E - commerce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KjBqWPk8yn_1HAN1CW1nnGC9eeYDz3SV

**Instacart Market Basket Analysis**
"""

#import all the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
a = pd.read_csv("/content/orders.csv")
b = pd.read_csv("/content/order_products__prior.csv")
c = pd.read_csv("/content/order_products__train.csv")
d = pd.read_csv("/content/products.csv")
e = pd.read_csv("/content/aisles.csv")
f = pd.read_csv("/content/departments.csv")

"""**Data Merging**

Since our goal is customer segmentation, we need to merge relevant datasets.

1ï¸âƒ£ Merge Orders with Products

ðŸ‘‰ First, combine orders.csv with order_products__prior.csv (since order_products__prior.csv has past orders, which are more useful than the training set).
"""

orders_products = b.merge(a, on="order_id", how="left")

"""Now, orders_products contains which user ordered what product.

2ï¸âƒ£ Merge Product Details

ðŸ‘‰ Now, add product names, aisles, and departments.
"""

orders_products = orders_products.merge(d, on="product_id", how="left")  # Add product names
orders_products = orders_products.merge(e, on="aisle_id", how="left")  # Add aisle info
orders_products = orders_products.merge(f, on="department_id", how="left")  # Add department info

"""âœ… Now, the dataset contains:

Customer behavior (user_id, order_dow, order_hour_of_day)

Product details (product_name, aisle, department)

Shopping frequency (days_since_prior_order)
"""

orders_products

"""**DATA EXPLORAITON AND CLEANING**"""

# check data types
orders_products.info()

# summary of the data
orders_products.describe()

# check for missing values
orders_products.isnull().sum()

# Handling Missing Values
# Fill missing values in 'add_to_cart_order' and 'reordered' with 0
orders_products['add_to_cart_order'].fillna(orders_products['add_to_cart_order'].mean(), inplace=True)
orders_products['reordered'].fillna(0, inplace=True)
orders_products['days_since_prior_order'].fillna(0, inplace=True)

#Check for duplicates
orders_products.duplicated().sum()

"""**Feature Engineering (Finalizing Columns for Clustering)**"""

orders_products

orders_products.columns

"""1. user_id (User ID)
What it is: Represents each individual customer.
What you can do: This is already an identifier for each customer. Youâ€™ll use it to group the data when calculating the features (e.g., total spend, number of orders, etc.).

2. order_id (Order ID)
What it is: This is the unique ID for each order.What you can do: You can count the number of orders per customer by grouping the data by user_id and counting the unique order_id values. This will give you an idea of how many times each customer has placed an order.

  Feature to create: num_orders (Number of Orders per Customer).
"""

num_orders = orders_products.groupby('user_id')['order_id'].nunique().reset_index()
num_orders.rename(columns={'order_id': 'num_orders'}, inplace=True)

"""3. add_to_cart_order (Position of Item in Cart)
What it is: The order of items in the customer's cart.
What you can do: This can help to determine the average basket size (how many items does a customer usually add to their cart).

   Feature to create: avg_basket_size (Average number of items per order).
"""

avg_basket_size = orders_products.groupby('user_id')['add_to_cart_order'].max().reset_index()
avg_basket_size.rename(columns={'add_to_cart_order': 'avg_basket_size'}, inplace=True)

"""4. reordered (Indicates whether a product was reordered)
What it is: A binary column indicating whether the product was ordered before.

   What you can do: Calculate the percentage of reordered items for each customer. This could indicate loyalty or preference for certain products.
   
   Feature to create: percent_reordered.
"""

percent_reordered = orders_products.groupby('user_id')['reordered'].mean().reset_index()
percent_reordered.rename(columns={'reordered': 'percent_reordered'}, inplace=True)

"""5. days_since_prior_order (Days since last purchase)
What it is: Number of days since the last order.
  
   What you can do: This is crucial for understanding customer recency. You can use it as-is, or if it contains missing values, you could fill those with 0 to mark first-time buyers (which we've discussed earlier).
   
   Feature to create: recency (the time since the last purchase).
"""

recency = orders_products.groupby('user_id')['days_since_prior_order'].last().reset_index()
recency.rename(columns={'days_since_prior_order': 'recency'}, inplace=True)

"""6. order_dow (Day of the Week)
What it is: This represents the day of the week when the order was placed.

   What you can do: You could create a frequency feature indicating how often a customer orders on each day of the week, or you could focus on a customer's preferred day for placing orders.

   Feature to create: preferred_day_of_week (most frequent order day).
"""

preferred_day = orders_products.groupby('user_id')['order_dow'].agg(lambda x: x.mode()[0]).reset_index()
preferred_day.rename(columns={'order_dow': 'preferred_day_of_week'}, inplace=True)

"""7. order_hour_of_day (Hour of the Day)
What it is: This represents the hour of the day when the order was placed.

   What you can do: Similar to order_dow, you can calculate the most frequent order time for each customer. This could help understand shopping patterns.
   
   Feature to create: preferred_order_time (most frequent order hour).
"""

preferred_time = orders_products.groupby('user_id')['order_hour_of_day'].agg(lambda x: x.mode()[0]).reset_index()
preferred_time.rename(columns={'order_hour_of_day': 'preferred_order_time'}, inplace=True)

"""8. aisle_id, department_id, aisle, department (Product Categories)
What it is: These columns represent the aisle and department to which a product belongs.

   What you can do: You could calculate the distribution of purchases in each department or aisle, helping to understand which categories the customer shops from most frequently.
   
   Feature to create: preferred_aisle, preferred_department (most frequent aisle or department).
"""

preferred_aisle = orders_products.groupby('user_id')['aisle'].agg(lambda x: x.mode()[0]).reset_index()
preferred_aisle.rename(columns={'aisle': 'preferred_aisle'}, inplace=True)

preferred_department = orders_products.groupby('user_id')['department'].agg(lambda x: x.mode()[0]).reset_index()
preferred_department.rename(columns={'department': 'preferred_department'}, inplace=True)

"""Final Feature Engineering Dataset:
After creating these features, you can merge them all into a single dataset for each customer. This dataset can then be used for clustering. Here's an example of how to combine everything:
"""

# Merge all feature dataframes into one
customer_data = pd.merge(num_orders, avg_basket_size, on='user_id')
customer_data = pd.merge(customer_data, percent_reordered, on='user_id')
customer_data = pd.merge(customer_data, recency, on='user_id')
customer_data = pd.merge(customer_data, preferred_day, on='user_id')
customer_data = pd.merge(customer_data, preferred_time, on='user_id')
customer_data = pd.merge(customer_data, preferred_aisle, on='user_id')
customer_data = pd.merge(customer_data, preferred_department, on='user_id')

# Now you have a table with one row per customer and their features for clustering

"""Summary of Features to Create:
Number of Orders per Customer (How many times a customer orders).

Average Basket Size (How many items do they usually buy).

Percentage of Reordered Items (Shows if the customer reorders items).

Recency (How recently did the customer make a purchase).

Preferred Day of the Week (What day do they usually shop).

Preferred Order Time (When do they usually place their orders).

Preferred Aisle/Department (What category or aisle do they shop from).
"""

customer_data

customer_data.columns

customer_data.info()

"""We have to convert object types into numeric types because machine can't handle categorical columns

Apply one-hot encoding
"""

customer_data = pd.get_dummies(customer_data, columns=['preferred_aisle', 'preferred_department'], drop_first=True)

# convert trur/false into 0/1
customer_data = customer_data.astype(int)

customer_data

"""See skewness in the data"""

skew = customer_data.skew()
print(skew[skew>2])

"""Handle skewness - Yeo-Johnson transformation"""

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method="yeo-johnson")
df = pt.fit_transform(customer_data)
# convert into a dataframe
df = pd.DataFrame(df, columns=customer_data.columns)

df.skew()

df

"""**Feature scaling**"""

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply Standard Scaling to your dataset (excluding any non-numeric columns if needed)
scaled_data = scaler.fit_transform(df)

# Convert it back to a DataFrame (optional)
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

# Check the result
print(scaled_df.head())

scaled_df.info()

scaled_df.shape

"""Dimensionality reduction - PCA

First, check how many components explain most of the variance.
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA()
X_pca = pca.fit_transform(df)

# Plot explained variance
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_.cumsum(), marker='o')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA - Explained Variance")
plt.grid()
plt.show()

import numpy as np

explained_variance = np.cumsum(pca.explained_variance_ratio_)  # Cumulative variance
num_components = np.argmax(explained_variance >= 0.95) + 1  # Find first index with >= 95% variance

print(f"Number of components needed for 95% variance: {num_components}")

num_components_90 = np.argmax(explained_variance >= 0.90) + 1
num_components_85 = np.argmax(explained_variance >= 0.85) + 1

print(f"Components for 90% variance: {num_components_90}")
print(f"Components for 85% variance: {num_components_85}")

"""We are going with 6 components"""

pca = PCA(n_components=6)
df_pca = pca.fit_transform(df)
print(df_pca.shape)  # Should be (198377, 123)

"""Convert it into a dataframe"""

df_pc = pd.DataFrame(df_pca)

df_pc

df_pc.shape

df_pc.dtypes

df_pc = df_pc.drop(columns=['clusters'])

"""**Model Training**

#### Kmeans - clustering

Elbow method - to check the best 'k'
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)  # Corrected 'init' value
    kmeans.fit(df_pc)
    wcss.append(kmeans.inertia_)

# Plot the WCSS to determine the optimal number of clusters (Elbow method)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-cluster Sum of Squares)')
plt.show()

"""we see elbow starts at 7, so we are using k=7"""

from sklearn.cluster import KMeans

#Applying 7 clusters
kmeans = KMeans(n_clusters=7, random_state=42)

#fit the model
kmeans.fit(df_pc)

# predict the clusters
# Predict the cluster labels for the data
df_pc['clusters'] = kmeans.predict(df_pc)

# Check the cluster distribution
print(df_pc['clusters'].value_counts())

"""the above shows Cluster 0: 98,576 samples,
Cluster 2: 32,801 samples,
Cluster 3: 27,753 samples,
Cluster 1: 20,000 samples,
Cluster 5: 11,913 samples,
Cluster 4: 5,721 samples,
Cluster 6: 1,613 samples.

Visualize the clusters
"""

import matplotlib.pyplot as plt

# Scatter plot of the first two PCA components
plt.figure(figsize=(10, 6))
plt.scatter(df_pc.iloc[:, 0], df_pc.iloc[:, 1], c=df_pc['clusters'], cmap='viridis', s=10, alpha=0.6)

# Mark the centroids of the clusters
centroids = kmeans.cluster_centers_  # The centroids from KMeans
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='x', label='Centroids')

# Add labels and color bar
plt.title("Cluster Distribution in PCA Reduced Space (6 components)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster')
plt.legend()

plt.show()

from sklearn.metrics import pairwise_distances

# Get the centroids of clusters 4 and 5
centroid_4 = kmeans.cluster_centers_[4]
centroid_5 = kmeans.cluster_centers_[5]

# Compute the distance between the centroids
distance = pairwise_distances([centroid_4], [centroid_5])
print("Distance between centroids 4 and 5:", distance)

"""**Evaluaiton Metrics -
1. Silhouette score
2. Davies - Bouldin Index
3. Inertia**

Silhouette Score: A silhouette score will help you determine how well the clustering algorithm has grouped the data. Higher scores indicate better-defined clusters, so evaluating the silhouette score can provide insights into the quality of your clusters.
"""

from sklearn.metrics import silhouette_score

# Calculate the silhouette score
silhouette_avg = silhouette_score(df_pc, kmeans.labels_)
print(f"Silhouette Score: {silhouette_avg}")

"""Davies-Bouldin Index - The Davies-Bouldin Index evaluates clustering performance based on the average similarity ratio of each cluster with the one most similar to it. A lower Davies-Bouldin score indicates better clustering."""

from sklearn.metrics import davies_bouldin_score

# Calculate Davies-Bouldin index for KMeans
dbi_score = davies_bouldin_score(df_pc, kmeans.labels_)
print(f"Davies-Bouldin Index: {dbi_score}")

"""Inertia (Within-Cluster Sum of Squares) - Inertia measures how internally coherent the clusters are. A lower inertia means that the clusters are tighter, but it should be used in conjunction with other metrics like the silhouette score. KMeans minimizes inertia during training, so comparing inertia at different values of k can help determine the best number of clusters."""

# Inertia (within-cluster sum of squares)
inertia = kmeans.inertia_
print(f"Inertia: {inertia}")

"""#### Hierarchial clustering

This technique builds a hierarchy of clusters by either a bottom-up (agglomerative) or top-down (divisive) approach.

Typically, Agglomerative Hierarchical Clustering is used, where each point starts as its own cluster, and pairs of clusters are merged as we move up the hierarchy. The most common way to evaluate the clusters formed is using a dendrogram, which shows the merging process and can help you decide the optimal number of clusters.
"""

# import all the required libraries
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# create dendrogram
plt.figure(figsize=(9,9))
sch.dendrogram(sch.linkage(df_pc, method='ward'))
plt.title('Dendrogram')

# Apply Agglomerative Clustering
agg_clustering = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')
df_pc['Agg_Cluster'] = agg_clustering.fit_predict(df_pc)

# Check the distribution of clusters
print(df_pc['Agg_Cluster'].value_counts())

# Scatter plot with clusters from Agglomerative Clustering
plt.figure(figsize=(10, 6))
plt.scatter(df_pc.iloc[:, 0], df_pc.iloc[:, 1], c=df_pc['Agg_Cluster'], cmap='viridis', s=10)
plt.title("Cluster Distribution using Hierarchical Clustering (7 clusters)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster')
plt.show()

from sklearn.metrics import silhouette_score

# Calculate the Silhouette Score
silhouette_avg = silhouette_score(df_pc, df_pc['Agg_Cluster'])
print(f"Silhouette Score (Agglomerative Clustering): {silhouette_avg}")

"""#### DBSCAN

Tune DBSCAN Parameters:
The key parameters to focus on are:

eps: Defines the maximum distance between two samples for them to be considered as neighbors.
min_samples: The minimum number of samples required to form a cluster.

Experiment with different values of eps and min_samples based on your dataset.
"""

from sklearn.cluster import DBSCAN

# Define DBSCAN with custom parameters
dbscan = DBSCAN(eps=0.3, min_samples=10)  # Adjust these based on your data

# Fit the DBSCAN model
df_pc['Cluster'] = dbscan.fit_predict(df_pc)

"""Analyze and Visualize the Results:
DBSCAN assigns -1 to noise points (points that don't belong to any cluster), so it's important to visualize both clusters and noise. Plot clusters with noise as distinct points.

Steps:
Use matplotlib to create scatter plots.
Color the noise points differently (those labeled -1).
"""

import matplotlib.pyplot as plt

# Visualizing clusters and noise
plt.figure(figsize=(10, 6))
plt.scatter(df_pc.iloc[:, 0], df_pc.iloc[:, 1], c=df_pc['Cluster'], cmap='viridis', s=10)

# Highlighting noise points
plt.scatter(df_pc.iloc[df_pc['Cluster'] == -1, 0], df_pc.iloc[df_pc['Cluster'] == -1, 1], c='red', marker='x', s=50, label='Noise')

plt.title("DBSCAN Clustering Results (with Noise)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster')
plt.legend()
plt.show()

"""Evaluate DBSCAN:
Since DBSCAN is density-based and does not require the number of clusters to be predefined, it is evaluated differently from methods like KMeans:

Cluster Counts: Check how many clusters and noise points were identified.
Outlier Identification: Noise points are marked with a cluster label of -1. Check how many noise points are present.
"""

# Checking the number of clusters and noise
print("Number of clusters found:", len(set(df_pc['Cluster'])) - (1 if -1 in df_pc['Cluster'].values else 0))
print("Number of noise points:", list(df_pc['Cluster']).count(-1))

# Cluster distribution
print(df_pc['Cluster'].value_counts())

"""Refine Parameters for Better Clusters:

If the initial DBSCAN parameters don't yield satisfactory results, adjust eps and min_samples:

1. Decrease eps: If you observe too many points being treated as noise, increase eps to merge them into clusters.

2. Increase min_samples: If too few points are grouped into clusters, increase min_samples to require a denser region to form a cluster.

You can run the model again with the updated parameters, ensuring the clusters are meaningful.

Optional: Visualize Dendrogram (Alternative to DBSCAN):

Sometimes, you can also use a hierarchical clustering approach as a comparison to DBSCAN. DBSCAN does not provide a hierarchical structure, but dendrograms help visualize relationships between points.
"""

import scipy.cluster.hierarchy as sch
# Apply hierarchical clustering and plot dendrogram
plt.figure(figsize=(10, 6))
dendrogram = sch.dendrogram(sch.linkage(df_pc, method='ward'))
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Index")
plt.ylabel("Distance")
plt.show()